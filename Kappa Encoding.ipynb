{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.39 s, sys: 7.82 s, total: 10.2 s\n",
      "Wall time: 521 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.mode.chained_assignment = None\n",
    "import logging\n",
    "from sklearn.base import clone\n",
    "import category_encoders as ce\n",
    "import importlib\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from IPython.display import display, HTML\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import skew\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import clone\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "random_state= 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global constants \n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_types(X):\n",
    "    categorical = []\n",
    "    ordinal = []\n",
    "    ordinal = X.select_dtypes('number').columns.to_list()\n",
    "    categorical = set(X.columns.to_list()) - set(ordinal)\n",
    "    return categorical,ordinal\n",
    "\n",
    "\n",
    "def normalize(X1):\n",
    "    result = X1.copy()\n",
    "    for feature_name in X1.columns:\n",
    "        max_value = X1[feature_name].max()\n",
    "        min_value = X1[feature_name].min()\n",
    "        result[feature_name] = X1[feature_name] / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "\n",
    "def preprocess_data(X_train, X_test, y_train, kappa, ordinal_columns):\n",
    "    X_train, X_test = handle_categorical(X_train, X_test, y_train, ordinal_columns)\n",
    "    X_train, X_test = handle_ordinal(X_train, X_test, y_train, kappa, ordinal_columns)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def handle_categorical(X_train, X_test, y_train, ordinal_columns):\n",
    "    means = {}\n",
    "\n",
    "    for col in X_train.columns:\n",
    "        if col not in ordinal_columns:  # Categorical variable\n",
    "            unique_values_train = X_train[col].unique()\n",
    "            unique_values_test = X_test[col].unique()\n",
    "\n",
    "            for val in unique_values_train:\n",
    "                mean = y_train[X_train[col] == val].mean()\n",
    "                means[(col, val)] = mean\n",
    "#                 print(f\"Column: {col}, Value: {val}, Mean: {mean}\")\n",
    "\n",
    "            for val in unique_values_test:\n",
    "                if val not in unique_values_train:\n",
    "                    print(f\"Warning: Value {val} in column {col} of X_test is not in X_train.\")\n",
    "\n",
    "    for col, val in means:\n",
    "        X_train.loc[X_train[col] == val, col] = means[(col, val)]\n",
    "        X_test.loc[X_test[col] == val, col] = means[(col, val)]\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def handle_ordinal(X_train, X_test, y_train, kappa, ordinal_columns):\n",
    "    missing_values = {}\n",
    "\n",
    "    for col in ordinal_columns:\n",
    "        unique_values_train = X_train[col].unique()\n",
    "        unique_values_test = X_test[col].unique()\n",
    "\n",
    "        y_train_np = y_train.to_numpy()\n",
    "        X_train_col_np = X_train[col].to_numpy()\n",
    "\n",
    "        for val in unique_values_test:\n",
    "            X_test_col_val = val\n",
    "            distances = np.abs(X_test_col_val - X_train_col_np)\n",
    "            weights = 1 / ((1 + distances) ** kappa)\n",
    "            l = np.sum(y_train_np * weights)\n",
    "            v = np.sum(weights)\n",
    "            imputed_value = l / v\n",
    "            missing_values[(col, val)] = imputed_value\n",
    "\n",
    "        for val in set(unique_values_train) - set(unique_values_test):\n",
    "            X_train_col_val = val\n",
    "            distances = np.abs(X_train_col_val - X_train_col_np)\n",
    "            weights = 1 / ((1 + distances) ** kappa)\n",
    "            l = np.sum(y_train_np * weights)\n",
    "            v = np.sum(weights)\n",
    "            imputed_value = l / v\n",
    "            missing_values[(col, val)] = imputed_value\n",
    "\n",
    "    for (col, val), imputed_value in missing_values.items():\n",
    "        X_train.loc[X_train[col] == val, col] = imputed_value\n",
    "        X_test.loc[X_test[col] == val, col] = imputed_value\n",
    "\n",
    "\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prop_reg(X_test,X_train,y_train,y_test,kappa):\n",
    "\n",
    "\n",
    "    d = np.linalg.norm(X_test[:, None] - X_train, axis=2)\n",
    "    weights = 1 / (1 + d) ** kappa\n",
    "    c = np.sum(y_train * weights, axis=1) / np.sum(weights, axis=1)\n",
    "\n",
    "    return(c)\n",
    "\n",
    "\n",
    "def kfold_cv(X, y, ordinal_columns, kappa_values_preprocessing, kappa_values_prop_reg, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "    results = {}\n",
    "    mae_values_per_kappa = {}\n",
    "\n",
    "    for kappa_preprocessing in kappa_values_preprocessing:\n",
    "        for kappa_prop_reg in kappa_values_prop_reg:\n",
    "           # kappa_prop_reg = kappa_preprocessing #short curcuit kapper_preprocess\n",
    "            mae_values = []\n",
    "\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "                X_train_preprocessed, X_test_preprocessed = preprocess_data(X_train, X_test, y_train, kappa_preprocessing, ordinal_columns)\n",
    "                y_pred = prop_reg(X_test_preprocessed.values, X_train_preprocessed.values, y_train.values, y_test.values, kappa_prop_reg)\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "                mae_values.append(mae)\n",
    "\n",
    "            results[(kappa_preprocessing, kappa_prop_reg)] = np.mean(mae_values)\n",
    "            mae_values_per_kappa[(kappa_preprocessing, kappa_prop_reg)] = mae_values\n",
    "\n",
    "    return results, mae_values_per_kappa\n",
    "\n",
    "def plots(kappa_values_prop_reg,kappa_values_preprocessing,mae_values_per_kappa):\n",
    "# Plot the MAE vs kappa curve for preprocessing\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for kappa_prop_reg in kappa_values_prop_reg:\n",
    "        mae_values = [np.mean(mae_values_per_kappa[(kappa_preprocessing, kappa_prop_reg)]) for kappa_preprocessing in kappa_values_preprocessing]\n",
    "        plt.plot(kappa_values_preprocessing, mae_values, '-o', label=f\"Prop_reg kappa: {kappa_prop_reg}\")\n",
    "\n",
    "    plt.xlabel('Preprocessing kappa')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.title('MAE vs Preprocessing kappa')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the MAE vs kappa curve for prop_reg\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for kappa_preprocessing in kappa_values_preprocessing:\n",
    "        mae_values = [np.mean(mae_values_per_kappa[(kappa_preprocessing, kappa_prop_reg)]) for kappa_prop_reg in kappa_values_prop_reg]\n",
    "        plt.plot(kappa_values_prop_reg, mae_values, '-o', label=f\"Preprocessing kappa: {kappa_preprocessing}\")\n",
    "\n",
    "    plt.xlabel('Prop_reg kappa')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.title('MAE vs Prop_reg kappa')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "    \n",
    "def optimal_kappa(cv_results):\n",
    "    # Find the optimal combination of kappa values\n",
    "\n",
    "    '''\n",
    "    {(kappa_inner,kapper_outer : score),}\n",
    "    '''\n",
    "    optimal_kappa_preprocessing, optimal_kappa_prop_reg = min(cv_results, key=cv_results.get)\n",
    "    optimal_mae = cv_results[(optimal_kappa_preprocessing, optimal_kappa_prop_reg)]\n",
    "\n",
    "    print(f\"Optimal Preprocessing kappa: {optimal_kappa_preprocessing}\")\n",
    "    print(f\"Optimal Prop_reg kappa: {optimal_kappa_prop_reg}\")\n",
    "    print(f\"Minimum MAE: {optimal_mae}\")\n",
    "    print('completed')\n",
    "    return optimal_kappa_preprocessing,optimal_kappa_prop_reg,optimal_mae\n",
    "\n",
    "\n",
    "def optimal_kappa_early_stopping(cv_results):\n",
    "    # Find the optimal combination of kappa values using early stopping\n",
    "    '''\n",
    "    {(kappa_inner,kapper_outer : score),}\n",
    "    '''\n",
    "    early_stopping = []\n",
    "    optimal_kappa_preprocessing, optimal_kappa_prop_reg = min(cv_results, key=cv_results.get)\n",
    "    optimal_mae = cv_results[(optimal_kappa_preprocessing, optimal_kappa_prop_reg)]\n",
    "    min_score = 0\n",
    "    last_item = 100000\n",
    "    for item in cv_results.items():\n",
    "        if item[0][0] == optimal_kappa_preprocessing:\n",
    "            if (last_item - item[1]) < 0.1:\n",
    "                    optimal_kappa_prop_reg =item[0][1]\n",
    "                    break\n",
    "            last_item = item[1]\n",
    "    print(f\"Optimal Preprocessing kappa: {optimal_kappa_preprocessing}\")\n",
    "    print(f\"Optimal Prop_reg kappa: {optimal_kappa_prop_reg}\")\n",
    "    print(f\"Minimum MAE: {optimal_mae}\")\n",
    "    print('completed')\n",
    "    return optimal_kappa_preprocessing,optimal_kappa_prop_reg,optimal_mae\n",
    "\n",
    "\n",
    "\n",
    "def find_kappa(X, y, ordinal_columns, kappa_values_preprocessing, kappa_values_prop_reg, n_splits=10):\n",
    "    cv_results, mae_values_per_kappa = kfold_cv(X, y, ordinal_columns, kappa_values_preprocessing, kappa_values_prop_reg, n_splits=n_splits)\n",
    "    a,b,c = optimal_kappa(cv_results)\n",
    "    plots(kappa_values_prop_reg,kappa_values_preprocessing,mae_values_per_kappa)\n",
    "    return a,b,c\n",
    "\n",
    "\n",
    "    \n",
    "def remove_outliers_zscore(df, threshold=3):\n",
    "\n",
    "    z_scores = np.abs((df - df.mean()) / df.std())\n",
    "    outlier_rows = z_scores.apply(lambda row: any(row > threshold), axis=1)\n",
    "    cleaned_df = df[~outlier_rows]\n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "\n",
    "def pearson_second_skewness(df, column_name):\n",
    "    column = df[column_name]\n",
    "    skewness = skew(column)\n",
    "    n = len(column)\n",
    "    skewness_p2 = (3 * skewness * (n - 1)**0.5) / (n - 2)\n",
    "    return skewness_p2\n",
    "\n",
    "def label_encode_columns(data, columns):\n",
    "   \n",
    "\n",
    "    for column_name in columns:\n",
    "        le = LabelEncoder()\n",
    "        data[column_name] = le.fit_transform(data[column_name])\n",
    "        \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     X1     X2     X3      X4   X5  X6   X7  X8\n",
      "0  0.98  514.5  294.0  110.25  7.0   2  0.0   0\n",
      "1  0.98  514.5  294.0  110.25  7.0   3  0.0   0\n",
      "2  0.98  514.5  294.0  110.25  7.0   4  0.0   0\n",
      "3  0.98  514.5  294.0  110.25  7.0   5  0.0   0\n",
      "4  0.90  563.5  318.5  122.50  7.0   2  0.0   0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "===============config================\n",
    "'''\n",
    "data = pd.read_csv('energy_data.csv')\n",
    "data = data.dropna()\n",
    "drop_data = [] # use this to drop any values\n",
    "drop_columns = ['Y2'] #use this to drop columns\n",
    "target_column = 'Y1'\n",
    "prescaling = True\n",
    "\n",
    "\n",
    "'''\n",
    "===============cleaning================\n",
    "'''\n",
    "\n",
    "for drop in drop_data:\n",
    "    data = data[~(data == drop).any(axis=1)]\n",
    "    \n",
    "if drop_columns != []: \n",
    "    data.drop(drop_columns, axis=1, inplace=True)\n",
    "    \n",
    "if target_column == -1:\n",
    "    y = data.iloc[:,-1]\n",
    "else:\n",
    "    y = data[target_column]\n",
    "\n",
    "'''\n",
    "===============modeling================\n",
    "'''\n",
    "\n",
    "X = data.drop(y.name, axis = 1)\n",
    "categorical_columns, ordinal_columns = get_types(X)\n",
    "\n",
    "X = label_encode_columns(X, categorical_columns)\n",
    "print(X.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits=5\n",
    "kf = KFold(n_splits=n_splits, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest kf MAE: 0.7901079416008827\n",
      "Decision Tree kf MAE: 0.8200913504795858\n",
      "K-Nearest Neighbors kf MAE: 2.995339653679654\n",
      "XGBoost kf MAE: 0.7181546597808379\n",
      "Multiple Linear Regression kf MAE: 2.281574055976061\n",
      "SVM kf MAE: 4.167284237372681\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(random_state=50),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=50),\n",
    "    'K-Nearest Neighbors': KNeighborsRegressor(n_neighbors=10),\n",
    "    'XGBoost': xgb.XGBRegressor(random_state=50),\n",
    "    'Multiple Linear Regression': LinearRegression(),\n",
    "    'SVM': SVR()\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=n_splits) \n",
    "\n",
    "for model_name, model in models.items():\n",
    "    mae_list = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        mae_list.append(mean_absolute_error(y_test, y_pred))\n",
    "    \n",
    "    print(f\"{model_name} kf MAE:\", np.mean(mae_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest kf MAE: 0.7833948238689413\n",
      "Decision Tree kf MAE: 0.816221220609456\n",
      "K-Nearest Neighbors kf MAE: 2.28728253968254\n",
      "XGBoost kf MAE: 0.7181546597808379\n",
      "Multiple Linear Regression kf MAE: 2.9784328831999347\n",
      "SVM kf MAE: 2.2886252254775563\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(random_state=50),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=50),\n",
    "    'K-Nearest Neighbors': KNeighborsRegressor(n_neighbors=10),\n",
    "    'XGBoost': xgb.XGBRegressor(random_state=50),\n",
    "    'Multiple Linear Regression': LinearRegression(),\n",
    "    'SVM': SVR()\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    mae_list = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        \n",
    "        X_train_normalized = scaler.fit_transform(X_train)\n",
    "\n",
    "        X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "        model.fit(X_train_normalized, y_train)\n",
    "        y_pred = model.predict(X_test_normalized)\n",
    "\n",
    "        mae_list.append(mean_absolute_error(y_test, y_pred))\n",
    "    \n",
    "    print(f\"{model_name} kf MAE:\", np.mean(mae_list))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components that explain 95% variance: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "num_components = np.where(cumulative_variance >= 0.95)[0][0] + 1  \n",
    "\n",
    "print(f\"Number of components that explain 95% variance: {num_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest kf MAE: 2.1798008904167725\n",
      "Decision Tree kf MAE: 2.722109175791529\n",
      "K-Nearest Neighbors kf MAE: 2.1340282998047706\n",
      "XGBoost kf MAE: 1.918178590916782\n",
      "Multiple Linear Regression kf MAE: 2.7105793896339967\n",
      "SVM kf MAE: 2.333242799936622\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(random_state=50),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=50),\n",
    "    'K-Nearest Neighbors': KNeighborsRegressor(n_neighbors=10),\n",
    "    'XGBoost': xgb.XGBRegressor(random_state=50),\n",
    "    'Multiple Linear Regression': LinearRegression(),\n",
    "    'SVM': SVR()\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5) \n",
    "\n",
    "\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "\n",
    "# Apply PCA on the scaled data\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "for model_name, model in models.items():\n",
    "    mae_list = []\n",
    "    for train_index, test_index in kf.split(X_pca):\n",
    "        X_train, X_test = X_pca[train_index], X_pca[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        mae_list.append(mean_absolute_error(y_test, y_pred))\n",
    "    \n",
    "    print(f\"{model_name} kf MAE:\", np.mean(mae_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_values = np.arange(0,10,0.05)  \n",
    "k_values = range(1,15,1)\n",
    "\n",
    "\n",
    "n_splits=5\n",
    "kf = KFold(n_splits=n_splits, random_state=0, shuffle=True)\n",
    "\n",
    "best_mae = float('inf')\n",
    "best_k = None\n",
    "best_kappa = None\n",
    "\n",
    "for kappa in kappa_values:\n",
    "    for k in k_values:\n",
    "        mae_list = []\n",
    "        \n",
    "        for train_index, val_index in kf.split(X):\n",
    "            X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "            y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "            \n",
    "            X_train_fold, X_val_fold = preprocess_data(X_train_fold, X_val_fold, y_train_fold, kappa, ordinal_columns)\n",
    "            #print(\"Preprocessed X_train_fold:\\n\", X_train_fold.head())\n",
    "            #print(\"\\nPreprocessed X_val_fold:\\n\", X_val_fold.head())\n",
    "            #print(\"NaN values in X_train_fold:\\n\", X_train_fold.isnull().sum())\n",
    "            #print(\"\\nNaN values in X_val_fold:\\n\", X_val_fold.isnull().sum())\n",
    "\n",
    "            \n",
    "            model = KNeighborsRegressor(n_neighbors=k)\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            y_pred = model.predict(X_val_fold)\n",
    "            \n",
    "            mae_list.append(mean_absolute_error(y_val_fold, y_pred))\n",
    "        \n",
    "        avg_mae = np.mean(mae_list)\n",
    "        if avg_mae < best_mae:\n",
    "            best_mae = avg_mae\n",
    "            best_k = k\n",
    "            best_kappa = kappa\n",
    "\n",
    "print(f\"Best Average MAE: {best_mae}, Best k: {best_k}, Best kappa: {best_kappa}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "models = [\n",
    "    (RandomForestRegressor(), \"Random Forest\"),\n",
    "    (DecisionTreeRegressor(), \"Decision Tree\"),\n",
    "    (SVR(kernel='linear'), \"SVM\"),\n",
    "    (LinearRegression(), \"Linear Regression\"),\n",
    "    (xgb.XGBRegressor(), \"XGBoost\")\n",
    "]\n",
    "\n",
    "kappa_values = np.arange(0, 10, 0.05)\n",
    "\n",
    "kf = KFold(n_splits=n_splits, random_state=0, shuffle=True)\n",
    "\n",
    "for model, model_name in models:\n",
    "    best_mae = float('inf')\n",
    "    best_kappa = None\n",
    "    \n",
    "    for kappa in kappa_values:\n",
    "        mae_list = []\n",
    "        \n",
    "        for train_index, val_index in kf.split(X):\n",
    "            X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "            y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "            \n",
    "            X_train_fold, X_val_fold = preprocess_data(X_train_fold, X_val_fold, y_train_fold, kappa, ordinal_columns)\n",
    "            \n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            y_pred = model.predict(X_val_fold)\n",
    "            \n",
    "            mae_list.append(mean_absolute_error(y_val_fold, y_pred))\n",
    "        \n",
    "        avg_mae = np.mean(mae_list)\n",
    "        if avg_mae < best_mae:\n",
    "            best_mae = avg_mae\n",
    "            best_kappa = kappa\n",
    "\n",
    "    print(f\"{model_name} - Best Average MAE: {best_mae}, Best kappa: {best_kappa}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
